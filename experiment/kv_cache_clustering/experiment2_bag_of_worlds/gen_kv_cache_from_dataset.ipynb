{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lyj/miniconda3/envs/LASKV/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "import pytz\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from datasets import load_dataset\n",
    "from pympler import asizeof\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since Salesforce/wikitext couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'wikitext-103-v1' at /home/lyj/.cache/huggingface/datasets/Salesforce___wikitext/wikitext-103-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3 (last modified on Tue Dec  3 03:28:42 2024).\n",
      "/home/lyj/miniconda3/envs/LASKV/lib/python3.8/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The rows of dataset:\n",
      "\ttrain:1801350\n",
      "\ttest:4358\n",
      "\tvalidation:3760\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# 加载数据集\n",
    "dataset = load_dataset(\"Salesforce/wikitext\", \"wikitext-103-v1\")\n",
    "print(\"The rows of dataset:\")\n",
    "print(f\"\\ttrain:{dataset['train'].num_rows}\")\n",
    "print(f\"\\ttest:{dataset['test'].num_rows}\")\n",
    "print(f\"\\tvalidation:{dataset['validation'].num_rows}\")\n",
    "\n",
    "# 加载预训练的分词器和模型\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
    "\n",
    "# GPT-2分词器没有pad_token，设置它\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_k_caches(k_caches, attention_mask):\n",
    "    # 计算batch中每个样本的序列长度\n",
    "    origin_seq_len = attention_mask.sum(dim=1)\n",
    "\n",
    "    # 创建掩码, 保留序列长度不为0的样本\n",
    "    remain_mask = (origin_seq_len != 0)\n",
    "    k_caches = [k[remain_mask] for k in k_caches]\n",
    "    origin_seq_len = origin_seq_len[remain_mask]\n",
    "\n",
    "    # 去除填充的padding的长度\n",
    "    filtered_k_caches = [[] for _ in range(len(k_caches))]\n",
    "    for layer_idx, k_cache in enumerate(k_caches):\n",
    "        # 沿着批次维度拆分tensor为list, (num_heads, seq_len, head_hidden_size)\n",
    "        k_cache_list = torch.unbind(k_cache, dim=0)\n",
    "        for sample_idx, sample_k_cache in enumerate(k_cache_list):\n",
    "            origin_sample_k_cache = sample_k_cache[:, :origin_seq_len[sample_idx], :]\n",
    "            filtered_k_caches[layer_idx].append(origin_sample_k_cache)\n",
    "\n",
    "    return filtered_k_caches\n",
    "\n",
    "def get_kv_cache(text, model, tokenizer, device):\n",
    "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True).to(device)\n",
    "    attention_mask = inputs['attention_mask']  # padding部分标记为0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, use_cache=True)\n",
    "    \n",
    "    # kv_caches: ((k_tensor, v_tensor), ...) 元组, 每个元素归属不同层\n",
    "    # k_tensor.shape: (batch, num_heads, seq_len, head_hidden_size)\n",
    "    kv_caches = outputs.past_key_values\n",
    "\n",
    "    k_caches = [kv[0] for kv in kv_caches]  # 获取所有层所有头的k cache\n",
    "    filtered_k_caches = filter_k_caches(k_caches, attention_mask)\n",
    "    \n",
    "    return filtered_k_caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_size_of_k_caches(all_k_caches):\n",
    "    size_threshold = 10000  # bytes = 10G\n",
    "    all_size = len(all_k_caches[0])\n",
    "    if all_size >= size_threshold:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def load_k_caches_to_file(all_k_caches, num_atten_layer):\n",
    "    directory_path = './k_caches'\n",
    "    if not os.path.exists(directory_path):\n",
    "        os.makedirs(directory_path)\n",
    "\n",
    "    # 获取当前东八区时间\n",
    "    cst = pytz.timezone('Asia/Shanghai')\n",
    "    now_cst = datetime.now(cst)\n",
    "    time_string = now_cst.strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "\n",
    "    file_name = f\"{directory_path}/k_cache_{time_string}.pkl\"\n",
    "    with open(file_name, 'wb') as f:\n",
    "        pickle.dump(all_k_caches, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "batch_size = 16\n",
    "num_atten_layer = len(model.transformer.h)\n",
    "all_k_caches = [[] for _ in range(num_atten_layer)]\n",
    "\n",
    "# for i in range(0, len(dataset['train']), batch_size):\n",
    "for i in range(0, len(dataset['train']), batch_size):\n",
    "    texts = dataset['train'][i:i + batch_size]['text']\n",
    "    \n",
    "    k_caches = get_kv_cache(texts, model, tokenizer, device)\n",
    "    for layer_idx, k_cache_layer in enumerate(k_caches):\n",
    "        for k_cache in k_cache_layer:\n",
    "            k_cache_cpu = k_cache.to('cpu').item\n",
    "            all_k_caches[layer_idx].append(k_cache_cpu)\n",
    "    del k_caches\n",
    "    \n",
    "    # 计算 all_k_caches 的大小, 达到阈值打包成文件\n",
    "    if check_size_of_k_caches(all_k_caches):\n",
    "        load_k_caches_to_file(all_k_caches, num_atten_layer)\n",
    "        del all_k_caches\n",
    "        all_k_caches = [[] for _ in range(num_atten_layer)]\n",
    "    \n",
    "    if int(i/batch_size)%20 == 0:\n",
    "        print(f\"Batch {int(i/batch_size)} has done, k cache size: {len(all_k_caches[0])}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LASKV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

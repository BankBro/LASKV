{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lyj/miniconda3/envs/LASKV/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "import pytz\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from datasets import load_dataset\n",
    "from pympler import asizeof\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since Salesforce/wikitext couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'wikitext-103-v1' at /home/lyj/.cache/huggingface/datasets/Salesforce___wikitext/wikitext-103-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3 (last modified on Tue Dec  3 03:28:42 2024).\n",
      "/home/lyj/miniconda3/envs/LASKV/lib/python3.8/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The rows of dataset:\n",
      "  train:1801350, test:4358, validation:3760\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# 加载数据集\n",
    "dataset = load_dataset(\"Salesforce/wikitext\", \"wikitext-103-v1\")\n",
    "print(\"The rows of dataset:\")\n",
    "print(f\"  train:{dataset['train'].num_rows}, test:{dataset['test'].num_rows}, validation:{dataset['validation'].num_rows}\")\n",
    "\n",
    "# 加载预训练的分词器和模型\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
    "\n",
    "# GPT-2分词器没有pad_token，设置它\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_embeddings(embeddings, attention_mask):\n",
    "    # 计算batch中每个样本的序列长度\n",
    "    origin_seq_len = attention_mask.sum(dim=1)\n",
    "\n",
    "    filtered_embeddings = []\n",
    "    for emb, seq_len in zip(embeddings, origin_seq_len):\n",
    "        if seq_len == 0:\n",
    "            continue\n",
    "        # 去除多余padding, tensor转为numpy\n",
    "        filtered_embeddings.append(emb[:seq_len].detach().cpu().numpy())\n",
    "\n",
    "    tokens_num = sum(origin_seq_len).item()\n",
    "    return filtered_embeddings, tokens_num\n",
    "\n",
    "def get_embeddings(text, model, tokenizer, device):\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        padding=True,  # 自动填充到最长序列长度\n",
    "        truncation=True,  # 截断超过最大长度的序列\n",
    "        max_length=5000,  # 设置最大长度\n",
    "        return_tensors='pt'  # 返回 PyTorch 张量\n",
    "    ).to(device)\n",
    "\n",
    "    attention_mask = inputs['attention_mask']  # padding部分标记为0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embeddings  = model.transformer.wte(inputs['input_ids'])\n",
    "    \n",
    "    return filter_embeddings(embeddings, attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current sample idx(0) has done.\n",
      "Current sample idx(8000) has done.\n",
      "Current sample idx(16000) has done.\n",
      "Current sample idx(24000) has done.\n",
      "All tokens num: 2070212.\n",
      "Current sample idx(32000) has done.\n",
      "Current sample idx(40000) has done.\n",
      "Current sample idx(48000) has done.\n",
      "Current sample idx(56000) has done.\n",
      "All tokens num: 4136703.\n",
      "Current sample idx(64000) has done.\n",
      "Current sample idx(72000) has done.\n",
      "Current sample idx(80000) has done.\n",
      "Current sample idx(88000) has done.\n",
      "All tokens num: 6182838.\n",
      "Current sample idx(96000) has done.\n",
      "Current sample idx(104000) has done.\n",
      "Current sample idx(112000) has done.\n",
      "Current sample idx(120000) has done.\n",
      "All tokens num: 8279582.\n",
      "Current sample idx(128000) has done.\n",
      "Current sample idx(136000) has done.\n",
      "Current sample idx(144000) has done.\n",
      "Current sample idx(152000) has done.\n",
      "All tokens num: 10335686.\n",
      "Current sample idx(160000) has done.\n",
      "Current sample idx(168000) has done.\n",
      "Current sample idx(176000) has done.\n",
      "Current sample idx(184000) has done.\n",
      "All tokens num: 12471960.\n",
      "Current sample idx(192000) has done.\n",
      "Current sample idx(200000) has done.\n",
      "Current sample idx(208000) has done.\n",
      "Current sample idx(216000) has done.\n",
      "All tokens num: 14590450.\n",
      "Current sample idx(224000) has done.\n",
      "Current sample idx(232000) has done.\n",
      "Current sample idx(240000) has done.\n",
      "Current sample idx(248000) has done.\n",
      "All tokens num: 16605704.\n",
      "Current sample idx(256000) has done.\n",
      "Current sample idx(264000) has done.\n",
      "Current sample idx(272000) has done.\n",
      "Current sample idx(280000) has done.\n",
      "All tokens num: 18707316.\n",
      "Current sample idx(288000) has done.\n",
      "Current sample idx(296000) has done.\n",
      "Current sample idx(304000) has done.\n",
      "Current sample idx(312000) has done.\n",
      "All tokens num: 20797100.\n",
      "Current sample idx(320000) has done.\n",
      "Current sample idx(328000) has done.\n",
      "Current sample idx(336000) has done.\n",
      "Current sample idx(344000) has done.\n",
      "All tokens num: 22871517.\n",
      "Current sample idx(352000) has done.\n",
      "Current sample idx(360000) has done.\n",
      "Current sample idx(368000) has done.\n",
      "Current sample idx(376000) has done.\n",
      "All tokens num: 24960693.\n",
      "Current sample idx(384000) has done.\n",
      "Current sample idx(392000) has done.\n",
      "Current sample idx(400000) has done.\n",
      "Current sample idx(408000) has done.\n",
      "All tokens num: 27100714.\n",
      "Current sample idx(416000) has done.\n",
      "Current sample idx(424000) has done.\n",
      "Current sample idx(432000) has done.\n",
      "Current sample idx(440000) has done.\n",
      "All tokens num: 29214486.\n",
      "Current sample idx(448000) has done.\n",
      "Current sample idx(456000) has done.\n",
      "Current sample idx(464000) has done.\n",
      "Current sample idx(472000) has done.\n",
      "All tokens num: 31306060.\n",
      "Current sample idx(480000) has done.\n",
      "Current sample idx(488000) has done.\n",
      "Current sample idx(496000) has done.\n",
      "Current sample idx(504000) has done.\n",
      "All tokens num: 33394725.\n",
      "Current sample idx(512000) has done.\n",
      "Current sample idx(520000) has done.\n",
      "Current sample idx(528000) has done.\n",
      "Current sample idx(536000) has done.\n",
      "All tokens num: 35502137.\n",
      "Current sample idx(544000) has done.\n",
      "Current sample idx(552000) has done.\n",
      "Current sample idx(560000) has done.\n",
      "Current sample idx(568000) has done.\n",
      "All tokens num: 37608813.\n",
      "Current sample idx(576000) has done.\n",
      "Current sample idx(584000) has done.\n",
      "Current sample idx(592000) has done.\n",
      "Current sample idx(600000) has done.\n",
      "All tokens num: 39695016.\n",
      "Current sample idx(608000) has done.\n",
      "Current sample idx(616000) has done.\n",
      "Current sample idx(624000) has done.\n",
      "Current sample idx(632000) has done.\n",
      "All tokens num: 41708547.\n",
      "Current sample idx(640000) has done.\n",
      "Current sample idx(648000) has done.\n",
      "Current sample idx(656000) has done.\n",
      "Current sample idx(664000) has done.\n",
      "All tokens num: 43769031.\n",
      "Current sample idx(672000) has done.\n",
      "Current sample idx(680000) has done.\n",
      "Current sample idx(688000) has done.\n",
      "Current sample idx(696000) has done.\n",
      "All tokens num: 45869375.\n",
      "Current sample idx(704000) has done.\n",
      "Current sample idx(712000) has done.\n",
      "Current sample idx(720000) has done.\n",
      "Current sample idx(728000) has done.\n",
      "All tokens num: 47971937.\n",
      "Current sample idx(736000) has done.\n",
      "Current sample idx(744000) has done.\n",
      "Current sample idx(752000) has done.\n",
      "Current sample idx(760000) has done.\n",
      "All tokens num: 50091915.\n",
      "Current sample idx(768000) has done.\n",
      "Current sample idx(776000) has done.\n",
      "Current sample idx(784000) has done.\n",
      "Current sample idx(792000) has done.\n",
      "All tokens num: 52233308.\n",
      "Current sample idx(800000) has done.\n",
      "Current sample idx(808000) has done.\n",
      "Current sample idx(816000) has done.\n",
      "Current sample idx(824000) has done.\n",
      "All tokens num: 54363025.\n",
      "Current sample idx(832000) has done.\n",
      "Current sample idx(840000) has done.\n",
      "Current sample idx(848000) has done.\n",
      "Current sample idx(856000) has done.\n",
      "All tokens num: 56497043.\n",
      "Current sample idx(864000) has done.\n",
      "Current sample idx(872000) has done.\n",
      "Current sample idx(880000) has done.\n",
      "Current sample idx(888000) has done.\n",
      "All tokens num: 58625038.\n",
      "Current sample idx(896000) has done.\n",
      "Current sample idx(904000) has done.\n",
      "Current sample idx(912000) has done.\n",
      "Current sample idx(920000) has done.\n",
      "All tokens num: 60723420.\n",
      "Current sample idx(928000) has done.\n",
      "Current sample idx(936000) has done.\n",
      "Current sample idx(944000) has done.\n",
      "Current sample idx(952000) has done.\n",
      "All tokens num: 62812994.\n",
      "Current sample idx(960000) has done.\n",
      "Current sample idx(968000) has done.\n",
      "Current sample idx(976000) has done.\n",
      "Current sample idx(984000) has done.\n",
      "All tokens num: 64848376.\n",
      "Current sample idx(992000) has done.\n",
      "Current sample idx(1000000) has done.\n",
      "Current sample idx(1008000) has done.\n",
      "Current sample idx(1016000) has done.\n",
      "All tokens num: 66939574.\n",
      "Current sample idx(1024000) has done.\n",
      "Current sample idx(1032000) has done.\n",
      "Current sample idx(1040000) has done.\n",
      "Current sample idx(1048000) has done.\n",
      "All tokens num: 69011181.\n",
      "Current sample idx(1056000) has done.\n",
      "Current sample idx(1064000) has done.\n",
      "Current sample idx(1072000) has done.\n",
      "Current sample idx(1080000) has done.\n",
      "All tokens num: 71103430.\n",
      "Current sample idx(1088000) has done.\n",
      "Current sample idx(1096000) has done.\n",
      "Current sample idx(1104000) has done.\n",
      "Current sample idx(1112000) has done.\n",
      "All tokens num: 73198160.\n",
      "Current sample idx(1120000) has done.\n",
      "Current sample idx(1128000) has done.\n",
      "Current sample idx(1136000) has done.\n",
      "Current sample idx(1144000) has done.\n",
      "All tokens num: 75305788.\n",
      "Current sample idx(1152000) has done.\n",
      "Current sample idx(1160000) has done.\n",
      "Current sample idx(1168000) has done.\n",
      "Current sample idx(1176000) has done.\n",
      "All tokens num: 77419505.\n",
      "Current sample idx(1184000) has done.\n",
      "Current sample idx(1192000) has done.\n",
      "Current sample idx(1200000) has done.\n",
      "Current sample idx(1208000) has done.\n",
      "All tokens num: 79544914.\n",
      "Current sample idx(1216000) has done.\n",
      "Current sample idx(1224000) has done.\n",
      "Current sample idx(1232000) has done.\n",
      "Current sample idx(1240000) has done.\n",
      "All tokens num: 81639851.\n",
      "Current sample idx(1248000) has done.\n",
      "Current sample idx(1256000) has done.\n",
      "Current sample idx(1264000) has done.\n",
      "Current sample idx(1272000) has done.\n",
      "All tokens num: 83698226.\n",
      "Current sample idx(1280000) has done.\n",
      "Current sample idx(1288000) has done.\n",
      "Current sample idx(1296000) has done.\n",
      "Current sample idx(1304000) has done.\n",
      "All tokens num: 85787738.\n",
      "Current sample idx(1312000) has done.\n",
      "Current sample idx(1320000) has done.\n",
      "Current sample idx(1328000) has done.\n",
      "Current sample idx(1336000) has done.\n",
      "All tokens num: 87850828.\n",
      "Current sample idx(1344000) has done.\n",
      "Current sample idx(1352000) has done.\n",
      "Current sample idx(1360000) has done.\n",
      "Current sample idx(1368000) has done.\n",
      "All tokens num: 89992526.\n",
      "Current sample idx(1376000) has done.\n",
      "Current sample idx(1384000) has done.\n",
      "Current sample idx(1392000) has done.\n",
      "Current sample idx(1400000) has done.\n",
      "All tokens num: 92108506.\n",
      "Current sample idx(1408000) has done.\n",
      "Current sample idx(1416000) has done.\n",
      "Current sample idx(1424000) has done.\n",
      "Current sample idx(1432000) has done.\n",
      "All tokens num: 94190808.\n",
      "Current sample idx(1440000) has done.\n",
      "Current sample idx(1448000) has done.\n",
      "Current sample idx(1456000) has done.\n",
      "Current sample idx(1464000) has done.\n",
      "All tokens num: 96339621.\n",
      "Current sample idx(1472000) has done.\n",
      "Current sample idx(1480000) has done.\n",
      "Current sample idx(1488000) has done.\n",
      "Current sample idx(1496000) has done.\n",
      "All tokens num: 98405321.\n",
      "Current sample idx(1504000) has done.\n",
      "Current sample idx(1512000) has done.\n",
      "Current sample idx(1520000) has done.\n",
      "Current sample idx(1528000) has done.\n",
      "All tokens num: 100522918.\n",
      "Current sample idx(1536000) has done.\n",
      "Current sample idx(1544000) has done.\n",
      "Current sample idx(1552000) has done.\n",
      "Current sample idx(1560000) has done.\n",
      "All tokens num: 102536615.\n",
      "Current sample idx(1568000) has done.\n",
      "Current sample idx(1576000) has done.\n",
      "Current sample idx(1584000) has done.\n",
      "Current sample idx(1592000) has done.\n",
      "All tokens num: 104599134.\n",
      "Current sample idx(1600000) has done.\n",
      "Current sample idx(1608000) has done.\n",
      "Current sample idx(1616000) has done.\n",
      "Current sample idx(1624000) has done.\n",
      "All tokens num: 106719905.\n",
      "Current sample idx(1632000) has done.\n",
      "Current sample idx(1640000) has done.\n",
      "Current sample idx(1648000) has done.\n",
      "Current sample idx(1656000) has done.\n",
      "All tokens num: 108820048.\n",
      "Current sample idx(1664000) has done.\n",
      "Current sample idx(1672000) has done.\n",
      "Current sample idx(1680000) has done.\n",
      "Current sample idx(1688000) has done.\n",
      "All tokens num: 110938695.\n",
      "Current sample idx(1696000) has done.\n",
      "Current sample idx(1704000) has done.\n",
      "Current sample idx(1712000) has done.\n",
      "Current sample idx(1720000) has done.\n",
      "All tokens num: 113041881.\n",
      "Current sample idx(1728000) has done.\n",
      "Current sample idx(1736000) has done.\n",
      "Current sample idx(1744000) has done.\n",
      "Current sample idx(1752000) has done.\n",
      "All tokens num: 115108852.\n",
      "Current sample idx(1760000) has done.\n",
      "Current sample idx(1768000) has done.\n",
      "Current sample idx(1776000) has done.\n",
      "Current sample idx(1784000) has done.\n",
      "All tokens num: 117209097.\n",
      "Current sample idx(1792000) has done.\n",
      "Current sample idx(1800000) has done.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided []",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 49\u001b[0m\n\u001b[1;32m     47\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     48\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m16\u001b[39m\n\u001b[0;32m---> 49\u001b[0m all_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mget_and_save_all_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 42\u001b[0m, in \u001b[0;36mget_and_save_all_embeddings\u001b[0;34m(model, tokenizer, dataset, batch_size, device)\u001b[0m\n\u001b[1;32m     38\u001b[0m batch_num \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2000\u001b[39m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]), batch_size \u001b[38;5;241m*\u001b[39m batch_num):\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# for i in range(0, 100000, batch_size * batch_num):\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m     tokens_num \u001b[38;5;241m=\u001b[39m \u001b[43mget_and_save_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_num\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m     all_tokens_num \u001b[38;5;241m=\u001b[39m all_tokens_num \u001b[38;5;241m+\u001b[39m tokens_num\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll tokens num: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mall_tokens_num\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 23\u001b[0m, in \u001b[0;36mget_and_save_embeddings\u001b[0;34m(model, tokenizer, dataset, start_idx, batch_size, batch_num, device)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cur_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_idx, end_idx, batch_size):\n\u001b[1;32m     22\u001b[0m     texts \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m][cur_idx : cur_idx \u001b[38;5;241m+\u001b[39m batch_size][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 23\u001b[0m     embeddings, tokens_num \u001b[38;5;241m=\u001b[39m \u001b[43mget_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     all_embeddings \u001b[38;5;241m=\u001b[39m all_embeddings \u001b[38;5;241m+\u001b[39m embeddings\n\u001b[1;32m     26\u001b[0m     all_tokens_num \u001b[38;5;241m=\u001b[39m all_tokens_num \u001b[38;5;241m+\u001b[39m tokens_num\n",
      "Cell \u001b[0;32mIn[3], line 16\u001b[0m, in \u001b[0;36mget_embeddings\u001b[0;34m(text, model, tokenizer, device)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_embeddings\u001b[39m(text, model, tokenizer, device):\n\u001b[0;32m---> 16\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 自动填充到最长序列长度\u001b[39;49;00m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 截断超过最大长度的序列\u001b[39;49;00m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 设置最大长度\u001b[39;49;00m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 返回 PyTorch 张量\u001b[39;49;00m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     24\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# padding部分标记为0\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/miniconda3/envs/LASKV/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2802\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2800\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2801\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2802\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2803\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2804\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m~/miniconda3/envs/LASKV/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2888\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2883\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2884\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch length of `text`: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not match batch length of `text_pair`:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2885\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text_pair)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2886\u001b[0m         )\n\u001b[1;32m   2887\u001b[0m     batch_text_or_text_pairs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(text, text_pair)) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m text\n\u001b[0;32m-> 2888\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2890\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2891\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2892\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2893\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2894\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2895\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2896\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2897\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2898\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2899\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2900\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2901\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2902\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2903\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2904\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2905\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2906\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2907\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2908\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_plus(\n\u001b[1;32m   2909\u001b[0m         text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[1;32m   2910\u001b[0m         text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2926\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2927\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/LASKV/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3079\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3069\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   3070\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   3071\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   3072\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3076\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3077\u001b[0m )\n\u001b[0;32m-> 3079\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3080\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3081\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3082\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3083\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3084\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3085\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3086\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3087\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3088\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3089\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3090\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3091\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3092\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3093\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3094\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3095\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3096\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3097\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/LASKV/lib/python3.8/site-packages/transformers/tokenization_utils.py:807\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    804\u001b[0m     second_ids \u001b[38;5;241m=\u001b[39m get_input_ids(pair_ids) \u001b[38;5;28;01mif\u001b[39;00m pair_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    805\u001b[0m     input_ids\u001b[38;5;241m.\u001b[39mappend((first_ids, second_ids))\n\u001b[0;32m--> 807\u001b[0m batch_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_prepare_for_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    809\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    810\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    811\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    813\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    814\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    815\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    816\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    817\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    818\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    819\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    820\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    821\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    822\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m BatchEncoding(batch_outputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/LASKV/lib/python3.8/site-packages/transformers/tokenization_utils.py:879\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._batch_prepare_for_model\u001b[0;34m(self, batch_ids_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_length, verbose)\u001b[0m\n\u001b[1;32m    876\u001b[0m             batch_outputs[key] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    877\u001b[0m         batch_outputs[key]\u001b[38;5;241m.\u001b[39mappend(value)\n\u001b[0;32m--> 879\u001b[0m batch_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    887\u001b[0m batch_outputs \u001b[38;5;241m=\u001b[39m BatchEncoding(batch_outputs, tensor_type\u001b[38;5;241m=\u001b[39mreturn_tensors)\n\u001b[1;32m    889\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m batch_outputs\n",
      "File \u001b[0;32m~/miniconda3/envs/LASKV/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3218\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.pad\u001b[0;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, return_attention_mask, return_tensors, verbose)\u001b[0m\n\u001b[1;32m   3216\u001b[0m \u001b[38;5;66;03m# The model's main input name, usually `input_ids`, has be passed for padding\u001b[39;00m\n\u001b[1;32m   3217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_input_names[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m encoded_inputs:\n\u001b[0;32m-> 3218\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3219\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou should supply an encoding or a list of encodings to this method \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3220\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthat includes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_input_names[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but you provided \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(encoded_inputs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3221\u001b[0m     )\n\u001b[1;32m   3223\u001b[0m required_input \u001b[38;5;241m=\u001b[39m encoded_inputs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_input_names[\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m   3225\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m required_input \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(required_input, Sized) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(required_input) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m):\n",
      "\u001b[0;31mValueError\u001b[0m: You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided []"
     ]
    }
   ],
   "source": [
    "def save_embeddings_to_file(embeddings, start_idx, end_idx):\n",
    "    directory_path = './embeddings'\n",
    "    if not os.path.exists(directory_path):\n",
    "        os.makedirs(directory_path)\n",
    "\n",
    "    # 获取当前东八区时间\n",
    "    cst = pytz.timezone('Asia/Shanghai')\n",
    "    now_cst = datetime.now(cst)\n",
    "    time_string = now_cst.strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "\n",
    "    file_name = f\"{directory_path}/embeddings_{start_idx}_{end_idx}_{time_string}.pkl\"\n",
    "    with open(file_name, 'wb') as f:\n",
    "        pickle.dump(embeddings, f)\n",
    "    \n",
    "\n",
    "def get_and_save_embeddings(model, tokenizer, dataset, start_idx, batch_size, batch_num, device):\n",
    "    end_idx = batch_size * batch_num + start_idx\n",
    "    all_embeddings = []\n",
    "    all_tokens_num = 0\n",
    "\n",
    "    for cur_idx in range(start_idx, end_idx, batch_size):\n",
    "        texts = dataset['train'][cur_idx : cur_idx + batch_size]['text']\n",
    "        embeddings, tokens_num = get_embeddings(texts, model, tokenizer, device)\n",
    "\n",
    "        all_embeddings = all_embeddings + embeddings\n",
    "        all_tokens_num = all_tokens_num + tokens_num\n",
    "\n",
    "        if int(cur_idx / batch_size) % 500 == 0:\n",
    "            print(f\"Current sample idx({cur_idx}) has done.\")\n",
    "\n",
    "    # 保存当前 (batch_num * batch) 个样本的 embeddings\n",
    "    save_embeddings_to_file(all_embeddings, start_idx, end_idx - 1)\n",
    "    return all_tokens_num\n",
    "\n",
    "\n",
    "def get_and_save_all_embeddings(model, tokenizer, dataset, batch_size, device):\n",
    "    all_tokens_num = 0\n",
    "    batch_num = 2000\n",
    "\n",
    "    for i in range(0, len(dataset['train']), batch_size * batch_num):\n",
    "    # for i in range(0, 100000, batch_size * batch_num):\n",
    "        tokens_num = get_and_save_embeddings(model, tokenizer, dataset, i, batch_size, batch_num, device)\n",
    "        all_tokens_num = all_tokens_num + tokens_num\n",
    "        print(f\"All tokens num: {all_tokens_num}.\")\n",
    "\n",
    "\n",
    "model.eval()\n",
    "batch_size = 16\n",
    "all_embeddings = get_and_save_all_embeddings(model, tokenizer, dataset, batch_size, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LASKV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
